---
layout: publication
authors:
  - Frank Elavsky
  - Cindy Xiong Bearfield
doi: 10.48550/arXiv.2508.12192
<!-- highlight: true -->
<!-- link: https://accessviz.github.io/ -->
<!-- html: https://ieeevis.org/year/2024/program/event_w-accessible.html -->
<!-- blog: https://bokeh-a11y-audit.readthedocs.io/ -->
pub: https://doi.org/10.48550/arXiv.2508.12192
pdf: https://www.frank.computer/papers/2025-vis-workshop-telephone.pdf
<!-- odf: mqzyx -->
<!-- short_doi: 10/f92f32 -->
<!-- slides: /talks/VegaLite-InfoVis-2016.pdf -->
tags:
  - Visualization
  - Accessibility
  - Bias
  - Machine learning
  - Generative AI
  - LLMs
title: Playing telephone with generative models: "verification disability," "compelled reliance," and accessibility in data visualization
<!-- tweet: We performed an in-depth accessibility evaluation for the interactive data visualization Python library, Bokeh. -->
type:
  - Workshop Paper
venue: IEEE VIS
venue_tags:
  - IEEE VIS
venue_url: https://ieeevis.org/
year: 2025
---

This paper is a collaborative piece between two worlds of expertise in the field of data visualization: accessibility and bias. In particular, the rise of generative models playing a role in accessibility is a worrying trend for data visualization. These models are increasingly used to help author visualizations as well as generate descriptions of existing visualizations for people who are blind, low vision, or use assistive technologies such as screen readers. Sighted human-to-human bias has already been established as an area of concern for theory, research, and design in data visualization. But what happens when someone is unable to verify the model output or adequately interrogate algorithmic bias, such as a context where a blind person asks a model to describe a chart for them? In such scenarios, trust from the user is not earned, rather reliance is compelled by the model-to-human relationship. In this work, we explored the dangers of AI-generated descriptions for accessibility, playing a game of telephone between models, observing bias production in model interpretation, and re-interpretation of a data visualization. We unpack ways that model failure in visualization is especially problematic for users with visual impairments, and suggest directions forward for three distinct readers of this piece: technologists who build model-assisted interfaces for end users, users with disabilities leveraging models for their own purposes, and researchers concerned with bias, accessibility, or visualization.
