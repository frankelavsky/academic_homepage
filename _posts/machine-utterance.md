---
layout: post
title: "Machine utterance: what does it mean for humanity to say something?"
slug: machine-utterance
tags:
- 
description: "If words are written, if sounds are uttered, was something still *said*? What does it mean to *say* anything at all? And in our day and age: if large-language models write and create for us, what are we (individuals or humanity) actually saying anymore?"
---

Alberto Cairo [posted on Linkedin](https://www.linkedin.com/posts/albertocairo_the-rise-of-genai-is-the-corollary-of-an-activity-7325474588681793537-GJcD?utm_source=social_share_send&utm_medium=member_desktop_web&rcm=ACoAADDAwBkBOdoW11I9B5DHy57VfR5jIs33Kq0) a provocation, "The rise of GenAI is the corollary of an era that values talking a lot without saying much."

In response, someone argued that AI may have "jagged edges" but is "certainly changing how things are done (for the better)."

Perhaps. I'm not sure if "for the better" here can be easily unpacked. I'm skeptical. So instead, I want to interrogate Alberto's words a bit more: what does it mean to "talk a lot without saying much?" And why does a shift from what "saying" something used to mean into what "saying" means now really matters?

## Generative AI is changing who we are: it is an *ontological* event
Now, I've already written (or should I say "said" here?) about how I think generative AI is shifting what roles we play as humans: from [having thoughts to managing them](https://www.frank.computer/blog/2024/06/llms-and-thoughts.html). But I think it could be argued that we do still learn *something* when we leverage AI tools. Human intellect is impressive and I'd argue that we aren't *not* learning when we use AI. But *what* we learn likely changes.

So instead perhaps what irks me are the impacts when we change from doing to managing. What do we get to claim is ours anymore?

My major provocation is this: agent-based AI do the talking, who is saying anything at all?

If I asked someone to give a speech at my wedding, I'm not the one saying something - they did. So I haven't really "said" anything. If I have an assistant who works for me and I ask them to ghost write a book for me, again, I haven't said anything. The ghost writer did.

But when we use AI agents to say, work, express, and do things - do we get to take credit simply because there isn't another human we are taking credit from? Why does asking, "write this email for me" mean we get credit for the email, if another entity (AI or person) wrote it?

So I'd argue that true users of AI agents aren't *saying* much of anything anymore. To have a thing written isn't the same as saying something. *Machine utterance isn't a human speaking*.

Users of AI agents are managers then, not doers.

One might learn things through the use of an AI agent, but claiming to have written a paper or code or whatever is dishonest and opaque; you may have, to some degree, *contributed* to the *production* of an artifact. To some degree, your management and validation may have been involved, but supervisors and QA testers aren't the same as engineers and designers.

Despite my protests and provocations, I do firmly believe that we are witnessing a definitional shift at a philosophical level. Generative AI and agent-based AI are changing what it means to "say" and "do" things. And I, quite realistically, don't think that I can defeat this new shift in language. One thing that humans have left to do, that we are still doing, is re-defining what it means to speak. I, of course, oppose this deeply. But again, I realistically am sure that I won't win. We already have "authors" who have never written whatever is in their own books. And this trend will likely continue so long as people profit from it.

So perhaps my final thoughts on this are on the ever-relevant critical question: who benefits and how?

In this definitional shift, "saying" used to mean that the thoughts, ideation, framing, motivations, editing, validation, expression, construction of language, and execution are performed by a human person who can be held responsible for all of the above and may take credit for all of the above.

## What is the real economy of modern artificial intelligences?
In a sense, there used to be a risk to writing. Writing and saying used to have an *economy* of give and take: you said things if it was worth it, to get away with it, to bring it into the world for your own benefit (or the benefit of others). We would speak to enact change, even if it had risk. This meant that we would sift and sort, the economy of writing, much like the economy of creation and craft, mattered to be done well. The cost of poor creation was wasted time, effort, and potential responsibility for damages caused or loss. Time, in particular, has always been one of the greatest filters of human production: we had to believe that creation would be worth our time. Responsibility, in tandem to time, was our primary method of refinement.

But now "saying" has drifted to mean that any one of the things that comprise the collective act of "saying" may or may not involve any human responsibility, time, or labor at all. Thoughts, ideation, framing, motivations, editing, validation, expression, construction of language, and execution may all be performed by machine agents. And because of that, we have lost responsibility. We have no more economy of creation. Writing something no longer has risk because it no longer requires anything of us.

The great selling point of generative AI, which is built on mountains and mountains of existing human creation, is that you no longer need to pay any price, save for 9.99 per month for a premium subscription, in order to create. Enough "hard" creation has already been done, now human labor can be re-created with ease. *Quite a tempting selling point!*

Interestingly, we have managed to still maintain an ethos where we can take credit for "saying" things, despite possibly having said nothing of our own at all anymore and not being responsible for what we've said, either.

And this leads us to the reason that I believe generative AI may be the tool that fully erodes our honesty and trust in digital spaces. Crystal Lee's research comes to mind: She has [an excellent piece on how viral visualizations were used to mislead and create disinformation](https://dl.acm.org/doi/abs/10.1145/3411764.3445211). What has happened in our modern age is that when we have economies without risk (which are, again, a fantastical proposition) and tools that enable those economies (such as ones that divorce humans from responsibility), we see that things like lying and destroying are now worth it.

Generative AI is, therefore, an enabler of this new fantasy economy, where machines "say" on our behalf and yet are capable of massive destruction. We pay nothing up front but a measly subscription fee. We have virtually no laws to regulate horribly evil acts like generative AI pornography of people without their consent, stealing the words and styling of writers and artists, producing books, research, data, and visualizations full of lies and falsehoods, eroding public and private trust in our existing infrastructures of knowledge, and so on.

## The real cost

And the worst "cost" of all that generative AI hides from us? The price of extraction being paid by our planet.

The cost to us right now seems low, but the price being paid is *very high*. It is an existential threat, in fact.

In my fantasy world, Braven, the big twist in the meta-narrative is that magic, which is accomplished by creating "portals" between our world and infinity, is actually just creating a portal to the future of our world. And eventually, that day arrives and immediately the planet of Braven is scorched to a crisp. Most all humanity dies instantly, all magic ceases to funciton, and the great demons who plotted patiently from under the earth finally emerge to consume the flesh of every burnt corpse that remains.

And I wrote this over 20 years ago as the cornerstone event of my whole world. And I watch as my own prophecy is coming true right now: we are bringing an apocalypse from a future we didn't need closer and closer to the present day, all because we have the convenience of "magic" at our hands.

What's left? Do we dismantle these systems?

Again, I'm realistic. People won't change their behavior unless we write laws and outline reasonable policies. Even the infamous creature of disinformation-enablement Joe Rogan believes that, for example, [contractors still need to have laws](https://www.youtube.com/shorts/N-wnLYBhrxY) or else all of our homes will fall apart.

People will cut corners over this next decade or two, politically, ditigally, physically, and intellectually. And houses, metaphorical or literal, *will* fall apart. And maybe we will have enough forensic threads and powerful enough arms of justice to respond when our calamities come knocking. But also, I'm a pessimist with technology. I think that the right will rise to power because of disinformation. They're willing to leverage economies and infrastructures that are dishonest and even illegal, if they know they can get away with it.

Perhaps my own solution will be to create more physical things, by hand, for a while. And I'll continue to write my own words, in the old way, and take responsibility for the things I've chosen to say because, for now, I still love myself enough to say and make my own things. We will see where this all goes in the coming years.

And maybe all of this is a motivation to leave academia, which is steeped in marriage to our new magics, and instead write my fiction, which is sadly becoming less and less fiction every day. If I'm lucky, I can get away with a stable job and still find the time to write. I feel like it is becoming more important than anything else now: to really *say* what should be said - about today, tomorrow, and worlds that we can only ever imagine existing, even if it costs me something to do so.